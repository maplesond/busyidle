<!DOCTYPE html>
<html>
  <head>
    <title>Title</title>
    <meta charset="utf-8">
    <style>
      @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
      @import url(https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

      body { font-family: 'Droid Serif'; }
      h1, h2, h3 {
        font-family: 'Yanone Kaffeesatz';
        font-weight: normal;
      }
      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
    </style>
  </head>
  <body>
    <textarea id="source">


### Outline

- Investigation into why CAP spinnaker deployments were failing
  - Found all pods on our spinnaker AKS cluster in *Unknown* state

- Hypothesis: overcommiting resources on a single node triggered a cascade of node failures in the cluster

- Demo:
  - Attempts to show that the hypothesis is correct ...

  - ... and how to avoid this happening again

- Also, in worst case, how can we recover from node failures?

---

### Demo

---

### Lessons Learnt

- *Always* specify resource request and limit values for every pods

- Monitoring and alerting is essential to identify problems before they occur
  
- Also consider:

  - Cluster auto-scaling (requires pod resource requests and limits to be set!)

  - Provising capable nodes; don't be too tight with VM size.  Keep in mind the default worker node from Azure (Standard_DS1_v2) only has 1 core and 2.7GB mem available to do work (3.5GB total).

  - Setting namespace resource value defaults: *request*, *limit*, *min*, and *max*.

  - Keeping memory request and limit levels the same.  (However, having a higher limit allows for pod to have occasionally memory bursts.)

---

### Recovering from node failure

This typically happens due to memory being overcommited

1. First type "*kubectl get po --all-namespaces -o wide*" to see state of pods and if these are specific to a node (look for pods in "*Unknown* state").

2. If there's a problem, try deleting deployments or pods that are running on that node

3. If that fails, start an internative shell running in the cluster then try *SSH*ing from there to the affected node.  If this works, kill any troublesome processes or consider restarting the node.

4. If that fails you probably need to delete a node via "*kubectl delete node ...*".  This can sometimes get out of sync with the AKS portal settings.  In that case you can try re-scaling the AKS cluster.

5. Failing that you need to pick up the phone to Microsoft support!
    


    </textarea>
    <script src="https://remarkjs.com/downloads/remark-latest.min.js">
    </script>
    <script>
      var slideshow = remark.create();
    </script>
  </body>
</html>
